
# GPU-Accelerated Matrix Multiplication & Image Convolution

## ğŸ§  Overview
This project explores **high-performance GPU computing** through **matrix multiplication** and **2D image convolution** using CUDA, cuBLAS, and Python integration via shared libraries.

I implemented and benchmarked multiple versions of matrix multiplication â€” from baseline CPU code to optimized CUDA kernels â€” and extended the work into **real-time image filtering** (blur and edge detection) accelerated through GPU parallelism.  

> ğŸ’¡ The goal: demonstrate how **GPU memory hierarchies, shared memory tiling, and kernel-level optimization** can yield thousand-fold speedups over traditional CPU implementations.


## âš™ï¸ Key Features

| Component | Description | Key Gains |
|------------|-------------|-----------|
| **CPU Matrix Multiply (C)** | Sequential baseline for runtime comparison | Baseline |
| **NaÃ¯ve CUDA Kernel** | One thread per output element | ~275Ã— faster than CPU at N=512 |
| **Tiled Shared Memory Kernel** | Introduced block tiling and data reuse | ~1.6Ã— faster than naÃ¯ve CUDA |
| **cuBLAS Integration** | Leveraged NVIDIAâ€™s optimized SGEMM routine | 7Ã— faster than custom kernel at 2048Ã—2048 |
| **CUDA Convolution** | GPU-accelerated blur and edge detection filters | Up to 130Ã— faster than CPU convolution |
| **Python Shared Library (`libconvolve.so`)** | Exposed CUDA kernels to Python for rapid prototyping | Seamless GPU acceleration in NumPy-style workflows |


## ğŸ“Š Performance Summary

### ğŸ§® Matrix Multiplication

| Implementation | N=512 | N=1024 | N=2048 |
|----------------|-------|--------|--------|
| **NaÃ¯ve CUDA** | 0.0012s | 0.0092s | 0.0753s |
| **CPU (C)** | 0.338s | 3.187s | 77.65s |
| **Tiled CUDA** | 0.00084s | 0.00589s | 0.0464s |
| **cuBLAS** | 0.00096s | 0.00163s | 0.00685s |

> âš™ï¸ **Result:** GPU achieved up to **10,000Ã— speedup** over CPU.  
> Shared-memory tiling yielded ~1.6Ã— faster runtime vs naÃ¯ve CUDA kernels.


### ğŸ–¼ï¸ Image Convolution

Performed 2D convolution on images up to **13,000Ã—13,000 pixels** for blur and edge detection filters.

| Filter Size (N) | CPU | CUDA Binary | CUDA Python Lib |
|-----------------|-----|-------------|-----------------|
| **3Ã—3** | 14.11s | 1.50s | 0.09s |
| **5Ã—5** | 17.08s | 1.50s | 0.11s |
| **7Ã—7** | 23.17s | 1.52s | 0.13s |

> ğŸ§© The Python wrapper version ran **>150Ã— faster than CPU** and **~10Ã— faster than native CUDA binaries**, thanks to efficient GPU memory handling and minimal host-device transfer overhead.


## ğŸ§© Implementation Details

### Matrix Multiplication Pipeline
1. **CPU Implementation:** Baseline C code using triple nested loops.  
2. **NaÃ¯ve CUDA:** 1 thread = 1 output element; global memory reads per multiply.  
3. **Tiled CUDA:** Used 16Ã—16 shared memory tiles to improve data locality.  
4. **cuBLAS:** Called `cublasSgemm()` for hardware-optimized GEMM.  

### Image Convolution
- Implemented custom **edge detection** and **blur** filters in C and CUDA.  
- Exposed both kernels through a **shared library (`libconvolve.so`)** using `extern "C"`.  
- Integrated GPU filtering into Python with `ctypes` and `NumPy`.  

## ğŸ”¬ Analysis & Learnings

- **GPU Parallelism Scales Exponentially:** Even at modest sizes, GPUs achieve 100Ã—â€“10,000Ã— speedups due to thousands of concurrent threads.  
- **Memory Optimization Dominates:** Tiling and shared memory reuse yield measurable gains beyond raw compute parallelism.  
- **cuBLAS Outperforms Custom Kernels:** Tensor cores, warp-level scheduling, and vectorized instructions give cuBLAS a clear advantage at scale.  
- **Python Integration Works:** Using `ctypes` and shared libraries allows lightweight ML pipelines to tap into CUDA performance without recompilation.  
- **Transfer Overhead Matters:** For small matrices or images, GPU copy time can dominate; efficient batching mitigates this.

## ğŸ§  Insights
> "Optimizing for GPUs isnâ€™t just about raw FLOPs â€” itâ€™s about *managing data flow, memory access, and parallel efficiency.*"

This project solidified my understanding of:
- CUDA memory hierarchy and thread/block design.  
- Trade-offs between compute intensity and data transfer overhead.  
- Extending C/CUDA performance into Python ML environments.


## ğŸ› ï¸ Tech Stack
**Languages:** C, CUDA C++, Python  
**Libraries:** cuBLAS, NumPy, ctypes  
**Hardware:** NVIDIA Tesla T4 / V100  
**Platform:** Google Cloud Compute Engine (Ubuntu 20.04)

## ğŸ§° Repository Structure
```bash
gpu-matrix-convolution/
â”œâ”€â”€ matrix_cpu.c              # CPU matrix multiplication
â”œâ”€â”€ matrix_gpu.cu             # NaÃ¯ve CUDA kernel
â”œâ”€â”€ matrix_tiled.cu           # Optimized tiled kernel
â”œâ”€â”€ matrix_cublas.cu          # cuBLAS implementation
â”œâ”€â”€ convolve_cuda.cu          # CUDA convolution (blur/edge)
â”œâ”€â”€ convolution_cpu_img.c     # CPU convolution baseline
â”œâ”€â”€ libconvolve.so            # Shared library for Python
â”œâ”€â”€ benchmark_all.py          # Python benchmarking script
â””â”€â”€ benchmark_v2.sh           # Bash script for matrix benchmarks
```

## ğŸ“ˆ Future Work
- Add **multi-GPU scaling** via CUDA streams and unified memory.  
- Integrate **cuDNN** for more advanced convolution pipelines.  
- Explore **quantized kernels** and TensorRT deployment for inference acceleration.  
- Build a **Flask-based web demo** showcasing GPU-accelerated image filters in real time.

